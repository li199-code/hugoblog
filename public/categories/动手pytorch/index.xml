<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>动手pytorch on JasonLee</title>
    <link>http://blog.jasonleehere.com/categories/%E5%8A%A8%E6%89%8Bpytorch/</link>
    <description>Recent content in 动手pytorch on JasonLee</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Tue, 06 Jun 2023 19:46:55 +0000</lastBuildDate><atom:link href="http://blog.jasonleehere.com/categories/%E5%8A%A8%E6%89%8Bpytorch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>针对csv文件类型的深度学习全流程记录</title>
      <link>http://blog.jasonleehere.com/posts/%E9%92%88%E5%AF%B9csv%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Tue, 06 Jun 2023 19:46:55 +0000</pubDate>
      
      <guid>http://blog.jasonleehere.com/posts/%E9%92%88%E5%AF%B9csv%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95/</guid>
      <description>前言 就像我之前的post里讲的那样，输入形式或者说处理对象的多样化是ai从业者面对的挑战之一。这篇文章，我将尽力解决存储在csv中的数据，涉</description>
    </item>
    
    <item>
      <title>Batch Normalization</title>
      <link>http://blog.jasonleehere.com/posts/normalization/</link>
      <pubDate>Sun, 02 Apr 2023 20:31:59 +0000</pubDate>
      
      <guid>http://blog.jasonleehere.com/posts/normalization/</guid>
      <description>前言 Batch Normalization （BN）层，通过将数据批量归一化（使其分布在N（0，1）），有下列好处： 缓解了梯度传递问题，使模型适应更大的学习率，加速了训练； 改善</description>
    </item>
    
    <item>
      <title>pytorch装环境</title>
      <link>http://blog.jasonleehere.com/posts/pytorch%E8%A3%85%E7%8E%AF%E5%A2%83/</link>
      <pubDate>Sat, 01 Apr 2023 20:54:38 +0000</pubDate>
      
      <guid>http://blog.jasonleehere.com/posts/pytorch%E8%A3%85%E7%8E%AF%E5%A2%83/</guid>
      <description>问题描述 跟李沐学AI，装环境时，发现无论是本地，还是切到colab，都出现环境问题。甚至还有奇葩的要求装gpu，但是conda返回的是cpu</description>
    </item>
    
    <item>
      <title>卷积</title>
      <link>http://blog.jasonleehere.com/posts/%E5%8D%B7%E7%A7%AF/</link>
      <pubDate>Thu, 30 Mar 2023 14:51:38 +0000</pubDate>
      
      <guid>http://blog.jasonleehere.com/posts/%E5%8D%B7%E7%A7%AF/</guid>
      <description>前言 我的研究生方向是图像处理，也用到了深度学习工具，所以这部分相对熟悉一些。在学习过程中，也发现了一些以前没有注意到的知识，比如卷积这个词是</description>
    </item>
    
    <item>
      <title>pytorch的基本使用</title>
      <link>http://blog.jasonleehere.com/posts/pytorch%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Tue, 28 Mar 2023 16:43:29 +0000</pubDate>
      
      <guid>http://blog.jasonleehere.com/posts/pytorch%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</guid>
      <description>之前我们有大量的从零实现一层网络，现在借用先进的pytorch框架，让很多功能得到封装，可以快捷的组建一个块。 nn.Module nn.Module是pyto</description>
    </item>
    
    <item>
      <title>神经网络初始化权重为什么不能为0</title>
      <link>http://blog.jasonleehere.com/posts/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8D%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E8%83%BD%E4%B8%BA0/</link>
      <pubDate>Tue, 28 Mar 2023 15:42:06 +0000</pubDate>
      
      <guid>http://blog.jasonleehere.com/posts/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8D%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E8%83%BD%E4%B8%BA0/</guid>
      <description>原来的困惑，权重是根据梯度大小来更新的，初始权重相同怎么会影响梯度更新呢？网上查阅答案，都说是什么损失函数根据权重大小进行分配，看的更是云里</description>
    </item>
    
    <item>
      <title>数值稳定性和权重初始化</title>
      <link>http://blog.jasonleehere.com/posts/%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%92%8C%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/</link>
      <pubDate>Mon, 27 Mar 2023 13:01:59 +0000</pubDate>
      
      <guid>http://blog.jasonleehere.com/posts/%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%92%8C%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/</guid>
      <description>梯度爆炸和梯度消失问题 梯度爆炸 因为梯度的计算是通过偏导数的链式法则，所以，对于一个很深的网络，反向传播时，计算最后几层的梯度，很可能会超出数</description>
    </item>
    
    <item>
      <title>多层感知机（MLP)</title>
      <link>http://blog.jasonleehere.com/posts/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BAmlp/</link>
      <pubDate>Sat, 25 Mar 2023 14:59:43 +0000</pubDate>
      
      <guid>http://blog.jasonleehere.com/posts/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BAmlp/</guid>
      <description>终于从前面的单层网络linear-regression和softmax过渡到多层神经网络了。为了对更加复杂的数据进行学习，多层感知机将多个全</description>
    </item>
    
    <item>
      <title>softmax回归（分类）</title>
      <link>http://blog.jasonleehere.com/posts/softmax%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB/</link>
      <pubDate>Thu, 23 Mar 2023 10:20:57 +0000</pubDate>
      
      <guid>http://blog.jasonleehere.com/posts/softmax%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB/</guid>
      <description>回归和分类 机器学习（深度学习）的任务纷繁复杂。最基础的是回归和分类。回归是预测连续值，分类是预测离散类别。 分类问题是多输出，因此，训练标签和</description>
    </item>
    
    <item>
      <title>线性回归</title>
      <link>http://blog.jasonleehere.com/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Wed, 22 Mar 2023 18:46:05 +0000</pubDate>
      
      <guid>http://blog.jasonleehere.com/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</guid>
      <description>线性回归 一句话表示，就是数据的分布是按照如下的线性表达式： $$ y=w_1x_1+w_2x_2+&amp;hellip;+w_nx_n+b $$ $w_n$就是网络的权重（参数），b 也是一种权重。 代码实现 import random import torch from d2l import torch as</description>
    </item>
    
    <item>
      <title>动手pytorch</title>
      <link>http://blog.jasonleehere.com/posts/%E5%8A%A8%E6%89%8Bpytorch/</link>
      <pubDate>Tue, 21 Mar 2023 10:18:17 +0000</pubDate>
      
      <guid>http://blog.jasonleehere.com/posts/%E5%8A%A8%E6%89%8Bpytorch/</guid>
      <description>这个系列记录我跟随 B 站上李沐的《动手学深度学习》的视频教程。每五个章节的笔记合成一篇 blog。 安装环境 我的学习平台时 win11, powershell, anaconda conda create -n d2l-zh python=3.8 conda activate d2l-zh pip</description>
    </item>
    
  </channel>
</rss>
