<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.119.0">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>线性回归 &middot; JasonLee</title>
  <meta name="description" content="" />

  
  <link type="text/css" rel="stylesheet" href="http://blog.jasonleehere.com/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="http://blog.jasonleehere.com/css/poole.css">
  <link type="text/css" rel="stylesheet" href="http://blog.jasonleehere.com/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="http://blog.jasonleehere.com/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  
</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="http://blog.jasonleehere.com/"><h1>JasonLee</h1></a>
      <p class="lead">
      An elegant open source and mobile first theme for <a href="http://hugo.spf13.com">hugo</a> made by <a href="http://twitter.com/mdo">@mdo</a>. Originally made for Jekyll.
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="http://blog.jasonleehere.com/">Home</a> </li>
        
      </ul>
    </nav>

    <p>&copy; 2023. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="post">
  <h1>线性回归</h1>
  <time datetime=2023-03-22T18:46:05Z class="post-date">Wed, Mar 22, 2023</time>
  <h2 id="线性回归">线性回归</h2>
<p>一句话表示，就是数据的分布是按照如下的线性表达式：</p>
<p>$$
y=w_1x_1+w_2x_2+&hellip;+w_nx_n+b
$$</p>
<p>$w_n$就是网络的权重（参数），b 也是一种权重。</p>
<h2 id="代码实现">代码实现</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> random
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> d2l <span style="color:#f92672">import</span> torch <span style="color:#66d9ef">as</span> d2l
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">## 自动生成数据集</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">synthetic_data</span>(w, b, num_examples):
</span></span><span style="display:flex;"><span>    X <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, (num_examples, len(w)))
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(X, w) <span style="color:#f92672">+</span> b
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">+=</span> torch<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0.01</span>, y<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> X, y<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>true_w <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">2</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">3.4</span>])
</span></span><span style="display:flex;"><span>true_b <span style="color:#f92672">=</span> <span style="color:#ae81ff">4.2</span>
</span></span><span style="display:flex;"><span>features, labels <span style="color:#f92672">=</span> synthetic_data(true_w, true_b, <span style="color:#ae81ff">1000</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>features<span style="color:#f92672">.</span>shape, labels<span style="color:#f92672">.</span>shape
</span></span></code></pre></div><pre><code>(torch.Size([1000, 2]), torch.Size([1000, 1]))
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">data_iter</span>(batch_size,features,labels):
</span></span><span style="display:flex;"><span>    num_examples<span style="color:#f92672">=</span>len(features)
</span></span><span style="display:flex;"><span>    indices<span style="color:#f92672">=</span>list(range(num_examples))
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#这些样本是随机读取的，没有特定的顺序</span>
</span></span><span style="display:flex;"><span>    random<span style="color:#f92672">.</span>shuffle(indices)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>,num_examples,batch_size):
</span></span><span style="display:flex;"><span>        batch_indices<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>tensor(indices[i:min(i<span style="color:#f92672">+</span>batch_size,num_examples)])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">yield</span> features[batch_indices],labels[batch_indices]
</span></span><span style="display:flex;"><span>batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> X,y <span style="color:#f92672">in</span> data_iter(batch_size,features,labels):
</span></span><span style="display:flex;"><span>    print(X,<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>,y)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">break</span>
</span></span></code></pre></div><pre><code>tensor([[-1.2750,  1.5482],
        [-0.0563, -2.0593],
        [-0.3648, -0.0083],
        [ 0.2933,  0.3219],
        [-0.6043, -0.0551],
        [-1.1544, -0.0258],
        [ 0.9690, -0.7872],
        [ 0.7860,  0.0937],
        [ 0.9102, -0.6743],
        [ 1.6593,  0.3044]])
 tensor([[-3.5984],
        [11.0968],
        [ 3.5161],
        [ 3.6972],
        [ 3.1954],
        [ 1.9791],
        [ 8.8084],
        [ 5.4522],
        [ 8.3228],
        [ 6.4840]])
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>w <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0.01</span>, size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>), requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">1</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">## 网络模型</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">linreg</span>(X, w, b):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>matmul(X, w) <span style="color:#f92672">+</span> b
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">## 损失函数</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">squared_loss</span>(y_hat, y):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> (y_hat <span style="color:#f92672">-</span> y<span style="color:#f92672">.</span>reshape(y_hat<span style="color:#f92672">.</span>shape))<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span><span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">## 优化算法（用来更新网络参数）</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sgd</span>(params, lr, batch_size):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> params:
</span></span><span style="display:flex;"><span>            param <span style="color:#f92672">-=</span> lr <span style="color:#f92672">*</span> param<span style="color:#f92672">.</span>grad <span style="color:#f92672">/</span> batch_size
</span></span><span style="display:flex;"><span>            param<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>zero_()
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">## 训练代码</span>
</span></span><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.03</span>
</span></span><span style="display:flex;"><span>num_epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>net <span style="color:#f92672">=</span> linreg
</span></span><span style="display:flex;"><span>loss <span style="color:#f92672">=</span> squared_loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(num_epochs):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> X, y <span style="color:#f92672">in</span> data_iter(batch_size, features, labels):
</span></span><span style="display:flex;"><span>        l <span style="color:#f92672">=</span> loss(net(X, w, b), y)
</span></span><span style="display:flex;"><span>        l<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>        sgd([w, b], lr, batch_size)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        train_l <span style="color:#f92672">=</span> loss(net(features, w, b), labels)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;open </span><span style="color:#e6db74">{</span>epoch<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, loss </span><span style="color:#e6db74">{</span>float(train_l<span style="color:#f92672">.</span>mean())<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span></code></pre></div><pre><code>open 1, loss 5.805317050544545e-05
open 2, loss 5.791278090327978e-05
open 3, loss 5.7983954320661724e-05
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;w的估计误差：</span><span style="color:#e6db74">{</span>true_w<span style="color:#f92672">-</span>w<span style="color:#f92672">.</span>reshape(true_w<span style="color:#f92672">.</span>shape)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;b的估计误差：</span><span style="color:#e6db74">{</span>true_b<span style="color:#f92672">-</span>b<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span></code></pre></div><pre><code>w的估计误差：tensor([-2.5511e-05, -1.8573e-04], grad_fn=`&lt;SubBackward0&gt;`)
b的估计误差：tensor([0.0001], grad_fn=`&lt;RsubBackward1&gt;`)
</code></pre>
<h3 id="文字说明">文字说明</h3>
<p>所有深度学习训练过程都可以用四个步骤概括：1.前向传播（输入到网络），2.计算损失（损失函数可导，且损失函数的值必须是标量），3.计算梯度（用于下一步的更新参数。backward()），4.更新网络参数（这里用的是 SGD）</p>
<h3 id="sgd">SGD</h3>
<p>gpt:</p>
<blockquote>
<p>随机梯度下降（Stochastic Gradient Descent，SGD）是一种常用的优化算法，常用于机器学习中的参数优化问题。在传统的梯度下降算法中，每次更新模型参数需要遍历整个训练集，计算所有样本的梯度平均值，这样的计算代价很大，尤其是在大规模数据集上。SGD 算法通过每次从训练集中随机选择一个样本进行梯度计算和模型参数更新，降低了计算代价，加快了模型收敛速度。此外，SGD 还可以避免陷入局部最优解，使得模型更有可能达到全局最优解。</p>
<p>然而，SGD 也有一些缺点，如对于数据的噪声敏感、容易受到初始点的影响等。因此，在实践中，常常使用一些改进的 SGD 算法，如动量优化、Adagrad、Adam 等来克服这些缺点。</p>
</blockquote>
<p>这段话里面。SGD 每次应该不一定只选择一个样本，而是若干个样本。这也因此引入了 batch_size 的概念。batch_size 就是每次计算梯度时选取样本的数量。</p>

</div>


    </main>

    
      
    
  </body>
</html>
