<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.119.0">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>python机器学习-数据降维 &middot; JasonLee</title>
  <meta name="description" content="" />

  
  <link type="text/css" rel="stylesheet" href="http://blog.jasonleehere.com/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="http://blog.jasonleehere.com/css/poole.css">
  <link type="text/css" rel="stylesheet" href="http://blog.jasonleehere.com/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="http://blog.jasonleehere.com/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  
</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="http://blog.jasonleehere.com/"><h1>JasonLee</h1></a>
      <p class="lead">
      An elegant open source and mobile first theme for <a href="http://hugo.spf13.com">hugo</a> made by <a href="http://twitter.com/mdo">@mdo</a>. Originally made for Jekyll.
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="http://blog.jasonleehere.com/">Home</a> </li>
        
      </ul>
    </nav>

    <p>&copy; 2023. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="post">
  <h1>python机器学习-数据降维</h1>
  <time datetime=2023-10-15T23:00:39Z class="post-date">Sun, Oct 15, 2023</time>
  <h2 id="pca主成分分析">PCA（主成分分析）</h2>
<p>example:</p>
<pre tabindex="0"><code>import numpy as np
from sklearn.decomposition import PCA

# 创建一个示例数据集
data = np.array([
    [1.5, 2.0, 3.0, 4.5],
    [2.2, 2.8, 3.9, 4.2],
    [1.9, 2.5, 3.2, 4.7],
    [2.7, 3.0, 3.6, 4.0],
    [2.3, 2.9, 3.7, 4.3]
])

# 初始化 PCA 模型，指定主成分的数量
pca = PCA(n_components=2)

# 对数据进行拟合
pca.fit(data)

# 获取主成分
principal_components = pca.components_

# 获取主成分的方差贡献比例
explained_variance_ratio = pca.explained_variance_ratio_

# 对数据进行 PCA 转换
transformed_data = pca.transform(data)

print(&#34;原始数据：\n&#34;, data)
print(&#34;主成分：\n&#34;, principal_components)
print(&#34;主成分的方差贡献比例：&#34;, explained_variance_ratio)
print(&#34;PCA 转换后的数据：\n&#34;, transformed_data)
</code></pre><p>输出：</p>
<pre tabindex="0"><code>原始数据：
 [[1.5 2.  3.  4.5]
 [2.2 2.8 3.9 4.2]
 [1.9 2.5 3.2 4.7]
 [2.7 3.  3.6 4. ]
 [2.3 2.9 3.7 4.3]]
主成分：
 [[-0.61305201 -0.55633419 -0.46392272  0.31533349]
 [-0.53989171 -0.03347232  0.83292811  0.11673604]]
主成分的方差贡献比例： [0.87919714 0.0703791 ]
PCA 转换后的数据：
 [[ 1.00928239 -0.02497259]
 [-0.37705186  0.28493986]
 [ 0.45617665 -0.0677326 ]
 [-0.71873459 -0.2649261 ]
 [-0.36967259  0.07269143]]
</code></pre><p>在深入之前，有必要解释一下 PCA 为什么在高维数据中找最大方差的方向而不是别的指标（比如最大值），来降维的？找到最大方差的方向意味着在这个方向上数据的变化最为显著，即信息越丰富。以我之前接触到的人脸图像为例，不同位置的像素有差异，才能让分类器有能力识别。</p>
<p>为了将我<a href="https://www.bilibili.com/video/BV1C7411A7bj/?spm_id_from=333.337.search-card.all.click&amp;vd_source=71c0be7c56c09a5e949353c5bf93df72">视频</a>学习到的和这个例子对应起来，将输出中的几个部分进行解释：</p>
<p>首先，这个例子中的原始数据是有五个样本，每个样本有四个维度的特征（一开始我还真没搞明白这个）。然后，pca 算法找出四个主成分（找的过程是通过计算协方差矩阵和特征向量等步骤得来的，视频举的例子是几何平面便于理解，而书上的公式是一种通用的方法，适合任意数量纬度特征，二者本质相同），也就是输出中的主成分。主成分本质上是一个方向向量，且模为 1，所以本例中的主成分有两组，每组四个值，对应四种特征，哪个特征的绝对值最大，说明哪个特征在这个主成分中占主导。方差贡献比例就是，依次计算所有数据在某一主成分上的投影点到距离之和，然后全部求和，最后计算一个主成分所占的比例，比例越高，说明方差越大，数据在这个主成分上差异越明显。最后的转换后数据，则是将所有数据在某一主成分上的投影点到距离，作为以各主成分为新坐标轴的空间内的坐标。</p>
<h2 id="lda线性判别分析">LDA（线性判别分析）</h2>
<p>不同于 PCA，LDA 有类别标签参与训练，所以是有监督的。LDA 训练过程中的优化目标也变成了最大化不同类之间的类间差距，而不是 PCA 的找样本内方差最大的方向，可以说是完全不同的方法，但是都能实现降维。</p>
<p>LDA 在一条线上投影数据点，并且让投影点的不同类之间的均值差距最大，而同类的方差最小，这样就让不同类之间的类间差距最大化。</p>
<p>可以说，LDA 的适用范围更窄些，它要求数据有明确的标签，且只适用分类问题；而 PCA 则适用范围更广，因为它只要求提供特征，能适用于不限于分类的问题。</p>
<p>example:</p>
<pre tabindex="0"><code># 导入必要的库
from sklearn import datasets
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
import matplotlib.pyplot as plt

# 加载示例数据集（这里以Iris数据集为例）
iris = datasets.load_iris()
X = iris.data
y = iris.target

# 创建LDA模型
lda = LinearDiscriminantAnalysis(n_components=2)  # 我们希望将数据降至二维

# 拟合模型并进行降维
X_lda = lda.fit_transform(X, y)

# 绘制降维后的数据
plt.figure(figsize=(8, 6))
colors = [&#39;navy&#39;, &#39;turquoise&#39;, &#39;darkorange&#39;]
lw = 2

for color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):
    plt.scatter(X_lda[y == i, 0], X_lda[y == i, 1], alpha=0.8, color=color,
                label=target_name)
plt.legend(loc=&#39;best&#39;, shadow=False, scatterpoints=1)
plt.title(&#39;LDA of IRIS dataset&#39;)
plt.show()
</code></pre><p><img src="https://fastly.jsdelivr.net/gh/li199-code/blog-imgs@main/16978904104791697890409581.png" alt="16978904104791697890409581.png"></p>

</div>


    </main>

    
      
    
  </body>
</html>
