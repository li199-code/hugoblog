<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.119.0">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>pytorch的基本使用 &middot; JasonLee</title>
  <meta name="description" content="" />

  
  <link type="text/css" rel="stylesheet" href="http://blog.jasonleehere.com/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="http://blog.jasonleehere.com/css/poole.css">
  <link type="text/css" rel="stylesheet" href="http://blog.jasonleehere.com/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="http://blog.jasonleehere.com/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  
</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="http://blog.jasonleehere.com/"><h1>JasonLee</h1></a>
      <p class="lead">
      An elegant open source and mobile first theme for <a href="http://hugo.spf13.com">hugo</a> made by <a href="http://twitter.com/mdo">@mdo</a>. Originally made for Jekyll.
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="http://blog.jasonleehere.com/">Home</a> </li>
        
      </ul>
    </nav>

    <p>&copy; 2023. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="post">
  <h1>pytorch的基本使用</h1>
  <time datetime=2023-03-28T16:43:29Z class="post-date">Tue, Mar 28, 2023</time>
  <p>之前我们有大量的从零实现一层网络，现在借用先进的pytorch框架，让很多功能得到封装，可以快捷的组建一个块。</p>
<h2 id="nnmodule">nn.Module</h2>
<p>nn.Module是pytorch一切网络的祖宗。例如，可以用nn.Sequential()搭建，也可以新写一个类：</p>
<pre tabindex="0"><code>class net(nn.Module):
   def __init__(self):
	super.__init__()
	...

   def forward(self, x):
	...
</code></pre><p>nn.Sequential和net类都是继承于nn.Module。</p>
<h2 id="参数管理">参数管理</h2>
<p>参数的访问用state_dict()函数：</p>
<pre tabindex="0"><code>print(net[2].state_dict())
</code></pre><pre tabindex="0"><code>OrderedDict([(&#39;weight&#39;, tensor([[ 0.3016, -0.1901, -0.1991, -0.1220,  0.1121, -0.1424, -0.3060,  0.3400]])), (&#39;bias&#39;, tensor([-0.0291]))])
</code></pre><p>每一种参数（比如weight）都是一个类，下面包含数值，梯度等属性。比如：</p>
<pre tabindex="0"><code>net[2].weight.data, net[2].weight.grad
</code></pre><h2 id="权重初始化">权重初始化</h2>
<p>默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵， 这个范围是根据输入和输出维度计算出的。 PyTorch的 <code>nn.init</code>模块提供了多种预置初始化方法。用nn.init：</p>
<pre tabindex="0"><code>def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)
net.apply(init_normal)
net[0].weight.data[0], net[0].bias.data[0]
</code></pre><p>m就是层的意思，apply函数会将init_normal函数遍历处理所有层。上述代码将所有的全连接层权重初始化为N~(0.0.01)的高斯分布。</p>
<h2 id="共享权重">共享权重</h2>
<p>共享权重似乎在论文中见过很多次，就是将一个子网络训练的过程中，将权重分享给另一个相同结构的子网络。这里给出的，是两个全连接层的参数共享：</p>
<pre tabindex="0"><code># 我们需要给共享层一个名称，以便可以引用它的参数
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.Linear(8, 1))
net(X)
# 检查参数是否相同
print(net[2].weight.data[0] == net[4].weight.data[0])
net[2].weight.data[0, 0] = 100
# 确保它们实际上是同一个对象，而不只是有相同的值
print(net[2].weight.data[0] == net[4].weight.data[0])
</code></pre><h2 id="创建全新的层">创建全新的层</h2>
<p>有时候，可能需要创建一个pytorch未实现的层。以自定义一个linear层为例：</p>
<pre tabindex="0"><code>class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)
</code></pre><p>核心步骤就是在init函数，设置好可以更新的权重，然后再forward函数里面定义计算方式。</p>
<h2 id="权重的保存checkpoint">权重的保存（checkpoint）</h2>
<p>pytorch只能保存权重，而不能连同网络结构一起保存，不过听说tf可以。保存方式是torch.save():</p>
<p>假设要保存参数的对应模型是一个MLP：</p>
<pre tabindex="0"><code>class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.output = nn.Linear(256, 10)

    def forward(self, x):
        return self.output(F.relu(self.hidden(x)))

net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)
</code></pre><pre tabindex="0"><code>torch.save(net.state_dict(), &#39;mlp.params&#39;) # 保存
</code></pre><pre tabindex="0"><code>net.load_state_dict(torch.load(&#39;mlp.params&#39;))  # 加载
</code></pre>
</div>


    </main>

    
      
    
  </body>
</html>
